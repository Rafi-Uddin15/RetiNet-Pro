{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Diabetic Retinopathy Detection: SOTA Transformer Edition (v2)\n",
                "\n",
                "**Goal**: >90% Accuracy & High Precision across ALL classes.\n",
                "\n",
                "### The \"SOTA\" Strategy:\n",
                "1.  **Model**: **Swin Transformer V2 Small** (Larger capacity than Tiny).\n",
                "2.  **Preprocessing**: **Ben Graham's Method**. This is the \"Secret Sauce\" used by Kaggle winners. It removes lighting variations and standardizes the retina.\n",
                "3.  **Loss Function**: **Focal Loss**. This forces the model to focus on \"Hard\" examples (like Mild DR) and ignore easy ones.\n",
                "4.  **Optimization**: WeightedRandomSampler + Mixup + Cutmix + TTA.\n",
                "\n",
                "### Instructions:\n",
                "1.  **Add Data**: Search for `diabetic-retinopathy-2015-data-colored-resized`.\n",
                "2.  **Accelerator**: GPU P100 or T4 x2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install \"numpy<2.0\" --upgrade timm torchmetrics grad-cam scipy scikit-learn\n",
                "\n",
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "import torchvision.transforms as transforms\n",
                "import timm\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything()\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Ben Graham's Preprocessing (The Secret Weapon)\n",
                "This function crops the black borders and applies a Gaussian blur subtraction to normalize lighting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_ben_color(path, sigmaX=10):\n",
                "    image = cv2.imread(path)\n",
                "    if image is None:\n",
                "        return np.zeros((256, 256, 3), dtype=np.uint8)\n",
                "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    # Crop black borders\n",
                "    # Convert to gray to find contours\n",
                "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
                "    mask = gray > 7\n",
                "    if mask.sum() == 0:\n",
                "        return cv2.resize(image, (256, 256))\n",
                "        \n",
                "    rows = np.any(mask, axis=1)\n",
                "    cols = np.any(mask, axis=0)\n",
                "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
                "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
                "    image = image[rmin:rmax, cmin:cmax]\n",
                "    \n",
                "    # Resize\n",
                "    image = cv2.resize(image, (256, 256))\n",
                "    \n",
                "    # Ben Graham's Method\n",
                "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX), -4, 128)\n",
                "    return image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "DATA_DIR = '/kaggle/input/diabetic-retinopathy-2015-data-colored-resized/colored_images/colored_images'\n",
                "data = []\n",
                "mapping = {'No_DR': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate_DR': 4}\n",
                "\n",
                "if os.path.exists(DATA_DIR):\n",
                "    for class_name, label in mapping.items():\n",
                "        class_dir = os.path.join(DATA_DIR, class_name)\n",
                "        if os.path.exists(class_dir):\n",
                "            for img_name in os.listdir(class_dir):\n",
                "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
                "                    data.append([os.path.join(class_dir, img_name), label])\n",
                "\n",
                "df = pd.DataFrame(data, columns=['id_code', 'label'])\n",
                "print(f\"Loaded {len(df)} images.\")\n",
                "\n",
                "if len(df) > 0:\n",
                "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
                "    \n",
                "    # Weighted Sampler\n",
                "    class_counts = df['label'].value_counts().sort_index().values\n",
                "    sample_weights = [1.0 / class_counts[label] for label in train_df['label']]\n",
                "    sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
                "else:\n",
                "    print(\"Dataset not found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RetinopathyDataset(Dataset):\n",
                "    def __init__(self, df, transform=None):\n",
                "        self.df = df\n",
                "        self.transform = transform\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        # Use Ben Graham Preprocessing here\n",
                "        image = load_ben_color(row['id_code'])\n",
                "        label = row['label']\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "        return image, torch.tensor(label, dtype=torch.long)\n",
                "\n",
                "train_transforms = transforms.Compose([\n",
                "    transforms.ToPILImage(),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(360),\n",
                "    transforms.ToTensor(),\n",
                "    # No normalization needed for Ben Graham? Actually standard ImageNet norm is still good practice\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "val_transforms = transforms.Compose([\n",
                "    transforms.ToPILImage(),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "if 'train_df' in locals():\n",
                "    train_dataset = RetinopathyDataset(train_df, transform=train_transforms)\n",
                "    val_dataset = RetinopathyDataset(val_df, transform=val_transforms)\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, sampler=sampler, num_workers=0, pin_memory=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Focal Loss\n",
                "Standard Cross Entropy is overwhelmed by easy examples. Focal Loss focuses training on hard negatives."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
                "        super(FocalLoss, self).__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "        self.reduction = reduction\n",
                "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = self.ce(inputs, targets)\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
                "\n",
                "        if self.reduction == 'mean':\n",
                "            return focal_loss.mean()\n",
                "        elif self.reduction == 'sum':\n",
                "            return focal_loss.sum()\n",
                "        else:\n",
                "            return focal_loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Swin Transformer V2 (Small)\n",
                "Upgrading from Tiny to Small for better feature extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SwinTransformerSOTA(nn.Module):\n",
                "    def __init__(self, num_classes=5, pretrained=True):\n",
                "        super(SwinTransformerSOTA, self).__init__()\n",
                "        # Upgrade to SMALL version: swinv2_small_window8_256.ms_in1k\n",
                "        self.backbone = timm.create_model('swinv2_small_window8_256.ms_in1k', pretrained=pretrained, num_classes=0)\n",
                "        num_features = self.backbone.num_features\n",
                "        \n",
                "        self.head = nn.Sequential(\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(num_features, 1024),\n",
                "            nn.BatchNorm1d(1024),\n",
                "            nn.Hardswish(),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(1024, num_classes)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.backbone(x)\n",
                "        x = self.head(x)\n",
                "        return x\n",
                "\n",
                "model = SwinTransformerSOTA(num_classes=5)\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training Setup\n",
                "from timm.data.mixup import Mixup\n",
                "from timm.loss import SoftTargetCrossEntropy\n",
                "\n",
                "# Mixup\n",
                "mixup_fn = Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, prob=1.0, switch_prob=0.5, mode='batch', label_smoothing=0.1, num_classes=5)\n",
                "\n",
                "# Loss: SoftTargetCrossEntropy for Mixup, FocalLoss for Validation/Clean\n",
                "criterion_train = SoftTargetCrossEntropy()\n",
                "criterion_val = FocalLoss(gamma=2)\n",
                "\n",
                "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.05) # Lower LR for larger model\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-7)\n",
                "scaler = GradScaler()\n",
                "\n",
                "def train_one_epoch(model, loader):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    pbar = tqdm(loader, desc=\"Training\")\n",
                "    for images, labels in pbar:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        images, labels = mixup_fn(images, labels)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        with autocast():\n",
                "            outputs = model(images)\n",
                "            loss = criterion_train(outputs, labels)\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        running_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': running_loss/len(loader)})\n",
                "    return running_loss / len(loader)\n",
                "\n",
                "def validate_tta(model, loader):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(loader, desc=\"Validation\"):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            # TTA: Original + Flip\n",
                "            out1 = model(images)\n",
                "            out2 = model(torch.flip(images, [3]))\n",
                "            outputs = (out1 + out2) / 2.0\n",
                "            \n",
                "            _, predicted = outputs.max(1)\n",
                "            total += labels.size(0)\n",
                "            correct += predicted.eq(labels).sum().item()\n",
                "    return correct / total\n",
                "\n",
                "# Run Training\n",
                "epochs = 40\n",
                "best_acc = 0.0\n",
                "\n",
                "if 'train_loader' in locals():\n",
                "    for epoch in range(epochs):\n",
                "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
                "        loss = train_one_epoch(model, train_loader)\n",
                "        acc = validate_tta(model, val_loader)\n",
                "        scheduler.step()\n",
                "        print(f\"Loss: {loss:.4f} | Val Acc: {acc:.4f}\")\n",
                "        \n",
                "        if acc > best_acc:\n",
                "            best_acc = acc\n",
                "            torch.save(model.state_dict(), 'best_model_sota.pth')\n",
                "            print(\"Saved Best Model!\")\n",
                "else:\n",
                "    print(\"No data loaded.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}