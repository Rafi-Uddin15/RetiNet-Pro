
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "try:\n",
                "    import timm\n",
                "    import torchmetrics\n",
                "    import pytorch_grad_cam\n",
                "except ImportError:\n",
                "    # Pin timm to 0.9.16 to avoid torch._dynamo circular import issues in some envs\n",
                "    !pip install \"numpy<2.0\" timm==0.9.16 torchmetrics grad-cam scipy scikit-learn --upgrade\n",
                "    print(\"Libraries installed. RESTARTING KERNEL to apply changes... (Please run this cell again after restart)\")\n",
                "    import os\n",
                "    os.kill(os.getpid(), 9)\n",
                "\n",
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm.auto import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "import torchvision.transforms as transforms\n",
                "import timm\n",
                "from timm.data.mixup import Mixup\n",
                "from timm.loss import SoftTargetCrossEntropy\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything()\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- PREPROCESSING: BEN GRAHAM ---\n",
                "def load_ben_color(path, sigmaX=10):\n",
                "    image = cv2.imread(path)\n",
                "    if image is None: return np.zeros((256, 256, 3), dtype=np.uint8)\n",
                "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
                "    mask = gray > 7\n",
                "    if mask.sum() == 0: return cv2.resize(image, (256, 256))\n",
                "        \n",
                "    rows = np.any(mask, axis=1)\n",
                "    cols = np.any(mask, axis=0)\n",
                "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
                "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
                "    image = image[rmin:rmax, cmin:cmax]\n",
                "    image = cv2.resize(image, (256, 256))\n",
                "    image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX), -4, 128)\n",
                "    return image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- CUSTOM MODEL: SWIN V2 SMALL + COORDINATE ATTENTION ---\n",
                "class CoordinateAttention(nn.Module):\n",
                "    def __init__(self, inp, reduction=32):\n",
                "        super(CoordinateAttention, self).__init__()\n",
                "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
                "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
                "\n",
                "        mip = max(8, inp // reduction)\n",
                "\n",
                "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
                "        self.bn1 = nn.BatchNorm2d(mip)\n",
                "        self.act = nn.Hardswish()\n",
                "        \n",
                "        self.conv_h = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)\n",
                "        self.conv_w = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        identity = x\n",
                "        n, c, h, w = x.size()\n",
                "        x_h = self.pool_h(x)\n",
                "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
                "\n",
                "        y = torch.cat([x_h, x_w], dim=2)\n",
                "        y = self.conv1(y)\n",
                "        y = self.bn1(y)\n",
                "        y = self.act(y) \n",
                "        \n",
                "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
                "        x_w = x_w.permute(0, 1, 3, 2)\n",
                "\n",
                "        a_h = self.conv_h(x_h).sigmoid()\n",
                "        a_w = self.conv_w(x_w).sigmoid()\n",
                "\n",
                "        out = identity * a_h * a_w\n",
                "        return out\n",
                "\n",
                "class SwinTransformerCA(nn.Module):\n",
                "    def __init__(self, num_classes=5, pretrained=True):\n",
                "        super(SwinTransformerCA, self).__init__()\n",
                "        # UPGRADE: Using 'Small' instead of 'Tiny' for SOTA performance\n",
                "        self.backbone = timm.create_model('swinv2_small_window8_256.ms_in1k', pretrained=pretrained, num_classes=0)\n",
                "        num_features = self.backbone.num_features\n",
                "        \n",
                "        self.ca = CoordinateAttention(num_features)\n",
                "        \n",
                "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.head = nn.Sequential(\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(num_features, 1024),\n",
                "            nn.BatchNorm1d(1024),\n",
                "            nn.Hardswish(),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(1024, num_classes)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.backbone.forward_features(x) # (B, H, W, C)\n",
                "        x = x.permute(0, 3, 1, 2) # (B, C, H, W)\n",
                "        x = self.ca(x)\n",
                "        x = self.avg_pool(x).flatten(1)\n",
                "        x = self.head(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STAGE 1: LOAD 2015 DATA ---\n",
                "DIR_2015 = '/kaggle/input/diabetic-retinopathy-2015-data-colored-resized/colored_images/colored_images'\n",
                "data_2015 = []\n",
                "mapping = {'No_DR': 0, 'Mild': 1, 'Moderate': 2, 'Severe': 3, 'Proliferate_DR': 4}\n",
                "\n",
                "if os.path.exists(DIR_2015):\n",
                "    for class_name, label in mapping.items():\n",
                "        d = os.path.join(DIR_2015, class_name)\n",
                "        if os.path.exists(d):\n",
                "            for f in os.listdir(d):\n",
                "                if f.endswith('.png'): data_2015.append([os.path.join(d, f), label])\n",
                "    df_2015 = pd.DataFrame(data_2015, columns=['path', 'label'])\n",
                "    print(f\"Stage 1 Data (2015): {len(df_2015)} images\")\n",
                "else:\n",
                "    print(\"2015 Dataset not found! Skipping Stage 1.\")\n",
                "    df_2015 = pd.DataFrame(columns=['path', 'label'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STAGE 2: LOAD APTOS 2019 DATA ---\n",
                "DIR_APTOS = '/kaggle/input/aptos2019-blindness-detection'\n",
                "if os.path.exists(DIR_APTOS):\n",
                "    df_aptos = pd.read_csv(os.path.join(DIR_APTOS, 'train.csv'))\n",
                "    df_aptos['path'] = df_aptos['id_code'].apply(lambda x: os.path.join(DIR_APTOS, 'train_images', x + '.png'))\n",
                "    df_aptos = df_aptos.rename(columns={'diagnosis': 'label'})\n",
                "    print(f\"Stage 2 Data (APTOS): {len(df_aptos)} images\")\n",
                "else:\n",
                "    print(\"APTOS Dataset not found! Cannot perform Stage 2.\")\n",
                "    df_aptos = pd.DataFrame(columns=['path', 'label'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DRDataset(Dataset):\n",
                "    def __init__(self, df, transform=None):\n",
                "        self.df = df\n",
                "        self.transform = transform\n",
                "    def __len__(self): return len(self.df)\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        img = load_ben_color(row['path'])\n",
                "        if self.transform: img = self.transform(img)\n",
                "        return img, torch.tensor(row['label'], dtype=torch.long)\n",
                "\n",
                "# --- ROBUSTNESS AUGMENTATION ---\n",
                "train_tf = transforms.Compose([\n",
                "    transforms.ToPILImage(),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(360),\n",
                "    # NEW: Mimic real-world noise/blur/lighting issues\n",
                "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
                "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
                "    transforms.RandomAutocontrast(p=0.3),\n",
                "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))], p=0.3),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "val_tf = transforms.Compose([\n",
                "    transforms.ToPILImage(), transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "mixup = Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, prob=1.0, switch_prob=0.5, mode='batch', label_smoothing=0.1, num_classes=5)\n",
                "criterion = SoftTargetCrossEntropy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, optimizer, scaler):\n",
                "    model.train()\n",
                "    avg_loss = 0\n",
                "    for img, lbl in tqdm(loader):\n",
                "        img, lbl = img.to(device), lbl.to(device)\n",
                "        img, lbl = mixup(img, lbl)\n",
                "        optimizer.zero_grad()\n",
                "        with autocast():\n",
                "            loss = criterion(model(img), lbl)\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        avg_loss += loss.item()\n",
                "    return avg_loss / len(loader)\n",
                "\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    with torch.no_grad():\n",
                "        for img, lbl in loader:\n",
                "            img, lbl = img.to(device), lbl.to(device)\n",
                "            # TTA\n",
                "            out = (model(img) + model(torch.flip(img, [3]))) / 2\n",
                "            correct += (out.argmax(1) == lbl).sum().item()\n",
                "            total += lbl.size(0)\n",
                "    return correct / total"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- EXECUTION ---\n",
                "model = SwinTransformerCA(num_classes=5).to(device)\n",
                "scaler = GradScaler()\n",
                "\n",
                "# STAGE 1: PRETRAIN (2015)\n",
                "if len(df_2015) > 0:\n",
                "    print(\"\\n=== STAGE 1: PRETRAINING ON 2015 DATASET ===\")\n",
                "    # Oversample 2015\n",
                "    weights = [1.0 / df_2015['label'].value_counts()[l] for l in df_2015['label']]\n",
                "    sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights), replacement=True)\n",
                "    \n",
                "    ds_train = DRDataset(df_2015, train_tf)\n",
                "    loader = DataLoader(ds_train, batch_size=32, sampler=sampler, num_workers=0)\n",
                "    \n",
                "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05)\n",
                "    \n",
                "    for ep in range(10): # 10 Epochs is enough for feature learning\n",
                "        loss = train_epoch(model, loader, optimizer, scaler)\n",
                "        print(f\"Stage 1 Epoch {ep+1}: Loss {loss:.4f}\")\n",
                "    \n",
                "    torch.save(model.state_dict(), 'pretrained_2015.pth')\n",
                "    print(\"Stage 1 Complete. Weights saved.\")\n",
                "\n",
                "# STAGE 2: FINETUNE (APTOS)\n",
                "if len(df_aptos) > 0:\n",
                "    print(\"\\n=== STAGE 2: FINE-TUNING ON APTOS 2019 ===\")\n",
                "    train_df, val_df = train_test_split(df_aptos, test_size=0.2, stratify=df_aptos['label'], random_state=42)\n",
                "    \n",
                "    # Oversample APTOS\n",
                "    weights = [1.0 / train_df['label'].value_counts()[l] for l in train_df['label']]\n",
                "    sampler = torch.utils.data.WeightedRandomSampler(weights, len(weights), replacement=True)\n",
                "    \n",
                "    train_loader = DataLoader(DRDataset(train_df, train_tf), batch_size=32, sampler=sampler, num_workers=0)\n",
                "    val_loader = DataLoader(DRDataset(val_df, val_tf), batch_size=32, shuffle=False, num_workers=0)\n",
                "    \n",
                "    # Lower LR for finetuning\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.05)\n",
                "    best_acc = 0\n",
                "    \n",
                "    for ep in range(20):\n",
                "        loss = train_epoch(model, train_loader, optimizer, scaler)\n",
                "        acc = validate(model, val_loader)\n",
                "        print(f\"Stage 2 Epoch {ep+1}: Loss {loss:.4f} | Val Acc {acc:.4f}\")\n",
                "        \n",
                "        if acc > best_acc:\n",
                "            best_acc = acc\n",
                "            torch.save(model.state_dict(), 'best_model_aptos.pth')\n",
                "            print(\"Saved Best APTOS Model!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- STAGE 3: EXPLAINABILITY (GRAD-CAM) ---\n",
                "from pytorch_grad_cam import GradCAM\n",
                "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
                "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def visualize_gradcam(model, img_path, label):\n",
                "    model.eval()\n",
                "    target_layers = [model.ca] # Target the Coordinate Attention layer\n",
                "    \n",
                "    # Load and Preprocess\n",
                "    img = load_ben_color(img_path)\n",
                "    input_tensor = val_tf(img).unsqueeze(0).to(device)\n",
                "    rgb_img = cv2.resize(img, (256, 256)) / 255.0\n",
                "    \n",
                "    # Get Prediction\n",
                "    with torch.no_grad():\n",
                "        output = model(input_tensor)\n",
                "        pred_idx = output.argmax(dim=1).item()\n",
                "        conf = torch.nn.functional.softmax(output, dim=1)[0][pred_idx].item()\n",
                "    \n",
                "    # Generate CAM for the TRUE LABEL (to see if model finds the lesions)\n",
                "    cam = GradCAM(model=model, target_layers=target_layers)\n",
                "    # Target the Ground Truth label to see \"What features support the true diagnosis?\"\n",
                "    grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(label)])[0]\n",
                "    visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
                "    \n",
                "    # Plot\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
                "    \n",
                "    # Original\n",
                "    axes[0].imshow(rgb_img)\n",
                "    axes[0].set_title(f\"Truth: {label} | Pred: {pred_idx} ({conf:.1%})\", fontsize=12, color='green' if label==pred_idx else 'red')\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    # Heatmap\n",
                "    axes[1].imshow(visualization)\n",
                "    axes[1].set_title(f\"Attention Map (Targeting Class {label})\", fontsize=12)\n",
                "    axes[1].axis('off')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Visualize a few samples from Validation Set (Focus on Severe Cases if possible)\n",
                "if len(df_aptos) > 0:\n",
                "    print(\"\\n=== GENERATING GRAD-CAM VISUALIZATIONS (Severe Cases) ===\")\n",
                "    # Load best model\n",
                "    model.load_state_dict(torch.load('best_model_aptos.pth'))\n",
                "    \n",
                "    # Try to pick samples from Class 3 (Severe) or 4 (Proliferative) to verify user concern\n",
                "    severe_samples = val_df[val_df['label'] >= 3]\n",
                "    if len(severe_samples) > 0:\n",
                "        samples = severe_samples.sample(min(3, len(severe_samples)))\n",
                "    else:\n",
                "        samples = val_df.sample(3)\n",
                "        \n",
                "    for idx, row in samples.iterrows():\n",
                "        visualize_gradcam(model, row['path'], row['label'])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
